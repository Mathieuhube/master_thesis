{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from pettingzoo.classic import texas_holdem_v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Texas Hold'em environment\n",
    "env = texas_holdem_v4.env(num_players=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_epsilon_greedy(Q, state, actions, epsilon=0.1):\n",
    "    state = tuple(state.items()) if isinstance(state, dict) else state  # Convert dict to tuple\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return random.choice(actions)  # Explore\n",
    "    return max(actions, key=lambda a: Q[state][a])  # Exploit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo with Exploring Starts\n",
    "class MCAgent:\n",
    "    def __init__(self, epsilon=0.1, gamma=1.0):\n",
    "        self.Q = defaultdict(lambda: defaultdict(float))  # Q[state][action] = value\n",
    "        self.returns = defaultdict(lambda: defaultdict(list))  # Track returns\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma  # Discount factor\n",
    "\n",
    "    def update_policy(self, episode):\n",
    "        G = 0\n",
    "        visited_states = set()\n",
    "        for state, action, reward in reversed(episode):\n",
    "            state = tuple(state.items()) if isinstance(state, dict) else state  # Convert dict to tuple\n",
    "            G = self.gamma * G + reward\n",
    "            if (state, action) not in visited_states:\n",
    "                self.returns[state][action].append(G)\n",
    "                self.Q[state][action] = np.mean(self.returns[state][action])\n",
    "                visited_states.add((state, action))\n",
    "\n",
    "    def train(self, num_episodes=10000):\n",
    "        for episode_num in range(num_episodes):\n",
    "            env.reset()\n",
    "            done = False\n",
    "            episode = []\n",
    "            for agent in env.agent_iter():\n",
    "                state, reward, termination, truncation, _ = env.last()\n",
    "                state = tuple(state.items()) if isinstance(state, dict) else state  # Convert dict to tuple\n",
    "                done = termination or truncation\n",
    "                available_actions = list(range(env.action_space(agent).n))\n",
    "                action = policy_epsilon_greedy(self.Q, state, available_actions, self.epsilon) if not done else None\n",
    "                env.step(action)\n",
    "                episode.append((state, action, reward))\n",
    "                if done:\n",
    "                    break\n",
    "            self.update_policy(episode)\n",
    "\n",
    "    def play(self):\n",
    "        env.reset()\n",
    "        done = False\n",
    "        for agent in env.agent_iter():\n",
    "            state, _, termination, truncation, _ = env.last()\n",
    "            state = tuple(state.items()) if isinstance(state, dict) else state  # Convert dict to tuple\n",
    "            done = termination or truncation\n",
    "            if done:\n",
    "                break\n",
    "            action = max(self.Q[state], key=self.Q[state].get, default=random.choice(range(env.action_space(agent).n)))\n",
    "            env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = MCAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m10000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mMCAgent.train\u001b[39m\u001b[34m(self, num_episodes)\u001b[39m\n\u001b[32m     28\u001b[39m done = termination \u001b[38;5;129;01mor\u001b[39;00m truncation\n\u001b[32m     29\u001b[39m available_actions = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(env.action_space(agent).n))\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m action = \u001b[43mpolicy_epsilon_greedy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mavailable_actions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     31\u001b[39m env.step(action)\n\u001b[32m     32\u001b[39m episode.append((state, action, reward))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mpolicy_epsilon_greedy\u001b[39m\u001b[34m(Q, state, actions, epsilon)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m random.uniform(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m) < epsilon:\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m random.choice(actions)  \u001b[38;5;66;03m# Explore\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mQ\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43ma\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mpolicy_epsilon_greedy.<locals>.<lambda>\u001b[39m\u001b[34m(a)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m random.uniform(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m) < epsilon:\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m random.choice(actions)  \u001b[38;5;66;03m# Explore\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmax\u001b[39m(actions, key=\u001b[38;5;28;01mlambda\u001b[39;00m a: \u001b[43mQ\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m]\u001b[49m[a])\n",
      "\u001b[31mTypeError\u001b[39m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "agent.train(10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
